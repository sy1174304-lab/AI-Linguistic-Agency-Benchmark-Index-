# AI-Linguistic-Agency-Benchmark-Index-
A sophisticated Python-based benchmarking tool to quantify the linguistic agency and cognitive maturity of AI models using Speech-Act-Theory and Semantic Understanding metrics.

# AI Linguistic-Agency-Benchmark-Index (ALABI)

The **AI Linguistic-Agency-Benchmark-Index** is a formal framework designed to categorize Artificial Intelligence systems based on their capacity for genuine linguistic agency. Unlike standard benchmarks that measure only accuracy, this index evaluates the "intent" and "understanding" behind the generated text.

Developed as part of the **Mohini Omega V410** research initiative.

## ðŸ“Œ Overview
This tool analyzes AI behavior across four critical dimensions to determine if a system is merely simulating speech or acting as a "Full-Asserter" with human-level comprehension.

## ðŸ›  Core Metrics (The Four Pillars)
* **Semantic Understanding:** Depth of meaning beyond simple pattern recognition.
* **Belief-like States:** Consistency of logic and persistent internal truth-values.
* **Theory of Mind:** The ability to model and respond to the mental states of others.
* **Normative Sanctionability:** The AI's accountability and adherence to linguistic rules.

## ðŸš€ Classification Tiers
The index classifies models into the following categories:
1.  **NON-ASSERTER:** Basic simulations (e.g., automated clocks, simple bots).
2.  **PROTO-ASSERTER:** LLMs with varying levels of understanding (Limited, Moderate, Advanced).
3.  **FULL-ASSERTER:** Human-level cognitive agency and complete understanding.

